{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNgK-CCskHGa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R60y8ddnCpye"
      },
      "source": [
        "Reading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81Danek_ka88"
      },
      "outputs": [],
      "source": [
        "tweet = pd.read_csv(r\"/content/train_data (1).csv\")\n",
        "# tweet = tweet[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvuCmIlak17k"
      },
      "outputs": [],
      "source": [
        "train_df = tweet\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX5nYK4mCtFt"
      },
      "source": [
        "Downloading the text file for word vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4nNKRSBm1N2"
      },
      "outputs": [],
      "source": [
        "!wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8exJGkvm4XS"
      },
      "outputs": [],
      "source": [
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WaF50IxC9Ve"
      },
      "source": [
        "Funtion to copy the word vector from text file to a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKfSFmualFoZ"
      },
      "outputs": [],
      "source": [
        "words = dict()\n",
        "\n",
        "def add_to_dict(d, filename):\n",
        "  with open(filename, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      line = line.split(' ')\n",
        "\n",
        "      try:\n",
        "        d[line[0]] = np.array(line[1:], dtype=float)\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "add_to_dict(words, 'glove.6B.50d.txt')\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozf8_dZcnwYC"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CjsyL-oDIzg"
      },
      "source": [
        "Funtion to convert tweets into word tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BW9PdO20oHWf"
      },
      "outputs": [],
      "source": [
        "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def message_to_token_list(s):\n",
        "  tokens = tokenizer.tokenize(s)\n",
        "  lowercased_tokens = [t.lower() for t in tokens]\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(t) for t in lowercased_tokens]\n",
        "  useful_tokens = [t for t in lemmatized_tokens if t in words]\n",
        "\n",
        "  return useful_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBgfUqqsDZsf"
      },
      "source": [
        "Function to convert wrod tokens into word vectors using vector dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xrYBGpPoWZP"
      },
      "outputs": [],
      "source": [
        "def message_to_word_vectors(message, word_dict=words):\n",
        "  processed_list_of_tokens = message_to_token_list(message)\n",
        "\n",
        "  vectors = []\n",
        "\n",
        "  for token in processed_list_of_tokens:\n",
        "    if token not in word_dict:\n",
        "      continue\n",
        "\n",
        "    token_vector = word_dict[token]\n",
        "    vectors.append(token_vector)\n",
        "\n",
        "  return np.array(vectors, dtype=float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCHrDqBPDkkG"
      },
      "source": [
        "Splitting the training data into 3 parts\n",
        "\n",
        "1.   Training data frame\n",
        "2.   Validation data frame\n",
        "3.   Test data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ9KwdO8oZqb"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.sample(frac=1, random_state=1)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "split_index_1 = int(len(train_df) * 0.6)\n",
        "split_index_2 = int(len(train_df) * 0.7)\n",
        "\n",
        "train_df, val_df, test_df = train_df[:split_index_1], train_df[split_index_1:split_index_2], train_df[split_index_2:]\n",
        "\n",
        "len(train_df), len(val_df), len(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuJI3VW9olkt"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gNqHWDrFGIL"
      },
      "source": [
        "Function to define the input/output data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB6bNz_Ro8jJ"
      },
      "outputs": [],
      "source": [
        "def df_to_X_y(dff):\n",
        "  y = dff['label'].to_numpy().astype(int)\n",
        "\n",
        "  all_word_vector_sequences = []\n",
        "\n",
        "  for message in dff['tweet']:\n",
        "    message_as_vector_seq = message_to_word_vectors(message)\n",
        "\n",
        "    if message_as_vector_seq.shape[0] == 0:\n",
        "      message_as_vector_seq = np.zeros(shape=(1, 50))\n",
        "\n",
        "    all_word_vector_sequences.append(message_as_vector_seq)\n",
        "\n",
        "  return all_word_vector_sequences, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez578lyFpHci"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = df_to_X_y(train_df)\n",
        "\n",
        "print(len(X_train), len(X_train[10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mi7SM2FQng"
      },
      "source": [
        "analysing the length of most tweet's useful tokens/vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Was5eb4pLTq"
      },
      "outputs": [],
      "source": [
        "sequence_lengths = []\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "  sequence_lengths.append(len(X_train[i]))\n",
        "\n",
        "pd.Series(sequence_lengths).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdYLTBi3FfaM"
      },
      "source": [
        "Padding the word token/vector list to convert them into numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAPiKLZgpUh_"
      },
      "outputs": [],
      "source": [
        "def pad_X(X, desired_sequence_length=59):\n",
        "  X_copy = deepcopy(X)\n",
        "\n",
        "  for i, x in enumerate(X):\n",
        "    x_seq_len = x.shape[0]\n",
        "    sequence_length_difference = desired_sequence_length - x_seq_len\n",
        "\n",
        "    pad = np.zeros(shape=(sequence_length_difference, 50))\n",
        "\n",
        "    X_copy[i] = np.concatenate([x, pad])\n",
        "\n",
        "  return np.array(X_copy).astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh06m3wTF0TU"
      },
      "source": [
        "padding the training data Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD0LIltRpbJg"
      },
      "outputs": [],
      "source": [
        "X_train = pad_X(X_train)\n",
        "\n",
        "X_train.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GLvZQx6F4G1"
      },
      "source": [
        "padding the validation data Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exAnzCSbvoV8"
      },
      "outputs": [],
      "source": [
        "X_val, y_val = df_to_X_y(val_df)\n",
        "X_val = pad_X(X_val)\n",
        "\n",
        "X_val.shape, y_val.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvF7hGJ5F809"
      },
      "source": [
        "padding the test data Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgWF5bc3FyNQ"
      },
      "outputs": [],
      "source": [
        "X_test, y_test = df_to_X_y(test_df)\n",
        "X_test = pad_X(X_test)\n",
        "\n",
        "X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj_gPE7rvos5"
      },
      "source": [
        "# **LSTM Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFIrCLHFGAuZ"
      },
      "source": [
        "Defining the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2lfxYQCpjXv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout,Bidirectional,SimpleRNN\n",
        "\n",
        "lstm_model = Sequential([])\n",
        "\n",
        "lstm_model.add(layers.Input(shape=(59, 50)))\n",
        "lstm_model.add(LSTM(64, return_sequences=True))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(LSTM(64, return_sequences=True))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(LSTM(64, return_sequences=True))\n",
        "lstm_model.add(Dropout(0.2))\n",
        "lstm_model.add(layers.Flatten())\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67ShEXd9GHTN"
      },
      "source": [
        "Compiling the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AOE614kpsdl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "cp = ModelCheckpoint('model/', save_best_only=True)\n",
        "\n",
        "lstm_model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss=BinaryCrossentropy(),\n",
        "              metrics=['accuracy', AUC(name='auc')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GssS2y-GMex"
      },
      "source": [
        "calculation for optimizing the model (as the dataset is not balanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PqA3n3cpvyh"
      },
      "outputs": [],
      "source": [
        "frequencies = pd.value_counts(train_df['label'])\n",
        "\n",
        "frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdNiz1T-pyJO"
      },
      "outputs": [],
      "source": [
        "weights = {0: frequencies.sum() / frequencies[0], 1: frequencies.sum() / frequencies[1]}\n",
        "weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPfR9gC5GWZM"
      },
      "source": [
        "Training the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7FXgqyEp0FK"
      },
      "outputs": [],
      "source": [
        "lstm_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, callbacks=[cp], class_weight = weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOMseHeHGkh2"
      },
      "source": [
        "Saving the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whqyzvhhp1pg"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "best_model = load_model('model/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmbuGHR0GrHs"
      },
      "source": [
        "Printing the trained model prediction report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmARwTGKp5Ch"
      },
      "outputs": [],
      "source": [
        "test_predictions = (best_model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, test_predictions))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
